{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "252daccb",
   "metadata": {},
   "source": [
    "# Project 3 Appendix: Comparison of Lemmatization and Stemming Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f0218b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0072428",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acc731ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in and assign dataframe\n",
    "crypto_df = pd.read_csv('datasets/crypto.csv')\n",
    "invest_df = pd.read_csv('datasets/invest.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05b237c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>full_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0, subreddit, author, title, selftext, created_utc, full_link]\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concate dataframes along columns\n",
    "combined_df = pd.concat(objs=[crypto_df, invest_df], axis=0)\n",
    "\n",
    "# find dupicate posts (cross posts) between both subreddits\n",
    "combined_df[combined_df.duplicated(['selftext'])] #no duplicate posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "333cfc0d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CryptoCurrency    1012\n",
       "investing         1004\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop individual index columns and create new index\n",
    "combined_df.reset_index(inplace=True, drop=True)\n",
    "combined_df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb1dc291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat title and selftext columns\n",
    "combined_df[\"all_text\"] = combined_df[\"title\"] + combined_df[\"selftext\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7b504c",
   "metadata": {},
   "source": [
    "# Lemmatization vs Stemming (PorterStemmer and WordNetLemmatizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc09383a",
   "metadata": {},
   "source": [
    "### Lemmatizer and Stemming Result Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c37908d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "p_stemmer = PorterStemmer()\n",
    "snow_stem = SnowballStemmer(language='english')\n",
    "\n",
    "def lemstem(row):\n",
    "\n",
    "    # Lemmatizing, Porter Stemming and Snowball Stemming\n",
    "    row['lemma_text'] = [lemmatizer.lemmatize(tok) for tok in row['all_text']]\n",
    "    row['porter_text'] = [p_stemmer.stem(tok) for tok in row['all_text']]\n",
    "    row['snow_text'] = [snow_stem.stem(tok) for tok in row['all_text']] \n",
    "    \n",
    "    return row\n",
    "\n",
    "lem_df = combined_df.apply(lemstem, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f7ef4029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binance binance binanc binanc\n",
      "assume assume assum assum\n",
      "title title titl titl\n",
      "says say say say\n",
      "couple couple coupl coupl\n",
      "weeks week week week\n",
      "especially especially especi especi\n",
      "since since sinc sinc\n",
      "started started start start\n",
      "aggressive aggressive aggress aggress\n",
      "marketing marketing market market\n",
      "large large larg larg\n",
      "volume volume volum volum\n",
      "overall overall overal overal\n",
      "presence presence presenc presenc\n",
      "noticed noticed notic notic\n",
      "different different differ differ\n",
      "statistics statistic statist statist\n",
      "different different differ differ\n",
      "analysis analysis analysi analysi\n",
      "websites website websit websit\n",
      "difference difference differ differ\n",
      "easy easy easi easi\n",
      "statistical statistical statist statist\n",
      "especially especially especi especi\n",
      "accuracy accuracy accuraci accuraci\n",
      "noticed noticed notic notic\n",
      "difference difference differ differ\n",
      "happening happening happen happen\n",
      "sites site site site\n",
      "remembered remembered rememb rememb\n",
      "binance binance binanc binanc\n",
      "purchased purchased purchas purchas\n",
      "decided decided decid decid\n",
      "investigation investigation investig investig\n",
      "statistics statistic statist statist\n",
      "binance binance binanc binanc\n",
      "ranking ranking rank rank\n",
      "able able abl abl\n",
      "based based base base\n",
      "ranking ranking rank rank\n",
      "binance binance binanc binanc\n",
      "reported reported report report\n",
      "volume volume volum volum\n",
      "reported reported report report\n",
      "statistics statistic statist statist\n",
      "surprisingly surprisingly surprisingli surpris\n",
      "independent independent independ independ\n",
      "website website websit websit\n",
      "reports report report report\n",
      "greatly greatly greatli great\n",
      "different different differ differ\n",
      "binance binance binanc binanc\n",
      "volume volume volum volum\n",
      "barely barely bare bare\n",
      "volume volume volum volum\n",
      "times time time time\n",
      "statistics statistic statist statist\n",
      "binance binance binanc binanc\n",
      "volume volume volum volum\n",
      "volume volume volum volum\n",
      "ranked ranked rank rank\n",
      "difference difference differ differ\n",
      "volume volume volum volum\n",
      "months month month month\n",
      "exchanges exchange exchang exchang\n",
      "centralized centralized central central\n",
      "earned earned earn earn\n",
      "refused refused refus refus\n",
      "validators validators valid valid\n",
      "regularly regularly regularli regular\n",
      "changed changed chang chang\n",
      "commission commission commiss commiss\n",
      "scamming scamming scam scam\n",
      "staking staking stake stake\n",
      "people people peopl peopl\n",
      "decentralised decentralised decentralis decentralis\n",
      "possible possible possibl possibl\n",
      "created created creat creat\n",
      "warning warning warn warn\n",
      "marked marked mark mark\n",
      "validators validators valid valid\n",
      "ranking ranking rank rank\n",
      "people people peopl peopl\n",
      "change change chang chang\n",
      "commission commission commiss commiss\n",
      "regularly regularly regularli regular\n"
     ]
    }
   ],
   "source": [
    "# comparison loop\n",
    "for original, lemma, porter, snow in zip(\n",
    "    lem_df['all_text'][0],\n",
    "    lem_df['lemma_text'][0], \n",
    "    lem_df['porter_text'][0], \n",
    "    lem_df['snow_text'][0]):\n",
    "    if (original != lemma) | ((lemma != porter) | (lemma != snow) | (porter != snow)):\n",
    "        print(original, lemma, porter, snow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b817b2",
   "metadata": {},
   "source": [
    "While not 100% accurate, using Lemmatization (resulting in the second word of each row) to normalise words seem to return the best outcome."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
